{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"poker-bot-dqn-limit-notebook.ipynb","provenance":[{"file_id":"1JWeHYPGubMrUHnNnvgtyFwvPz-ZYlW_l","timestamp":1605439097828}],"collapsed_sections":[],"authorship_tag":"ABX9TyO1PPMt5N92dmsphBVVQrRH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aRJsfnUrmS_H","executionInfo":{"status":"ok","timestamp":1605602457113,"user_tz":-60,"elapsed":10104,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"8593db48-4d74-401c-c333-b6fa00b8c6be","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install git+https://github.com/datamllab/rlcard"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/datamllab/rlcard\n","  Cloning https://github.com/datamllab/rlcard to /tmp/pip-req-build-ql5h2fc_\n","  Running command git clone -q https://github.com/datamllab/rlcard /tmp/pip-req-build-ql5h2fc_\n","Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.18.5)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (3.2.2)\n","Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (7.0.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (20.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard==0.2.6) (1.15.0)\n","Building wheels for collected packages: rlcard\n","  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rlcard: filename=rlcard-0.2.6-cp36-none-any.whl size=6785365 sha256=ba484343f3a6a58840ce3de85078b5a32587e2cc1f05c9b6324adde4a97154a0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cn65tpp1/wheels/b3/e1/32/6535ad7ff9142e4c031af97e237e4df3e4ab14e86194738ac4\n","Successfully built rlcard\n","Installing collected packages: rlcard\n","Successfully installed rlcard-0.2.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V9-pPwFQmjxb","executionInfo":{"status":"ok","timestamp":1605602462997,"user_tz":-60,"elapsed":5940,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"9cb349f3-c2a1-40a3-b2d5-5621ca0a940b","colab":{"base_uri":"https://localhost:8080/"}},"source":["%tensorflow_version 1.x # for using tensorflow.contrib\n","import tensorflow as tf"],"execution_count":2,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x # for using tensorflow.contrib`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_DjHYr9NmBE8","executionInfo":{"status":"ok","timestamp":1605602465373,"user_tz":-60,"elapsed":1097,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["from collections import namedtuple\n","import random\n","import numpy as np\n","\n","Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n","\n","class ReplayMemory(object):\n","    ''' \n","    Replay memory for saving transitions\n","    '''\n","    def __init__(self, capacity, batch_size):\n","        ''' \n","        Initialize ReplayMemory\n","\n","        :param int capacity: the size of the memory buffer\n","        :param int batch_size: the size of the batches\n","        '''\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        '''\n","        Save a transition into memory\n","        '''\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        '''\n","        Choose random sample from the memory with size of the batch size\n","        '''\n","        samples = random.sample(self.memory, batch_size)\n","        return map(np.array, zip(*samples))\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJOP-_FYlz_X","executionInfo":{"status":"ok","timestamp":1605602470607,"user_tz":-60,"elapsed":3961,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","class DQN_network(object):\n","    '''\n","    Deep Q-Network\n","    '''\n","\n","    def __init__(self, state_no=36, act_no=4, hidden_layers=[64, 32], learning_rate=0.001, device=None):\n","        ''' \n","        Initilalize the DQN_network object.\n","\n","        :param act_no (int): Number of actions (4 in Leduc Hold'em)\n","        :param state_no (list): Size of the state space (36 in Leduc Hold'em)\n","        :param hidden_layers (list): Dimension of the hidden layers\n","        :param device (torch.device): Usage CPU or GPU\n","        '''\n","        self.state_no = state_no\n","        self.act_no = act_no\n","        self.hidden_layers = hidden_layers\n","        self.learning_rate=learning_rate\n","        self.device = device\n","\n","        # DQN network based on the layers\n","        layers = self.state_no + self.hidden_layers\n","        DQN_network = [nn.Flatten()]\n","        DQN_network.append(nn.BatchNorm1d(layers[0]))\n","        for i in range(len(layers)-1):\n","            DQN_network.append(nn.Linear(layers[i], layers[i+1], bias=True))\n","            DQN_network.append(nn.Tanh())\n","        DQN_network.append(nn.Linear(layers[-1], self.act_no, bias=True))\n","        DQN_network = nn.Sequential(*DQN_network)\n","\n","        DQN_network = DQN_network.to(self.device)\n","        self.DQN_network = DQN_network\n","        self.DQN_network.eval()\n","\n","        # Initialize weights in the network\n","        for p in self.DQN_network.parameters():\n","            if len(p.data.shape) > 1:\n","                nn.init.xavier_uniform_(p.data)\n","\n","        # Define loss function\n","        self.loss_function = nn.MSELoss(reduction='mean')\n","\n","        # Define optimizer\n","        #self.optimizer =  torch.optim.Adam(self.DQN_network.parameters(), lr=self.learning_rate)\n","        self.optimizer = torch.optim.RMSprop(self.DQN_network.parameters())\n","\n","\n","    def get_qvalue(self, next_state_batch):\n","        ''' \n","        Get Q-values for the batch of the next states.\n","        It does not use gradient calculation.\n","\n","        :param np.ndarray next_state_batch: Batch of the next states\n","        :return np.ndarray Q_values: The estimated Q-values\n","        '''\n","        # Disable gradient calculation\n","        with torch.no_grad():\n","            # Create torch tensor\n","            next_state_batch = torch.from_numpy(next_state_batch).float().to(self.device)\n","            # Get Q values\n","            Q_values = self.DQN_network(next_state_batch).cpu().numpy()\n","        return Q_values\n","\n","    def update(self, state_batch, action_batch, target_batch):\n","        ''' \n","        Update the policy network\n","\n","        :param np.ndarray state_batch: Batch of states from replay memory\n","        :param np.ndarray action_batch: Batch of actions from replay memory\n","        :param np.ndarray target_batch: Batch of Q-values from the target policy, it used during the optimization step\n","        :return float batch_loss: The calculated loss on the batch       \n","        '''\n","        # Set the gradients to zero\n","        self.optimizer.zero_grad()\n","\n","        # Set the network in training mode\n","        self.DQN_network.train()\n","\n","        # Create torch tensors\n","        state_batch = torch.from_numpy(state_batch).float().to(self.device)\n","        action_batch = torch.from_numpy(action_batch).long().to(self.device)\n","        target_batch = torch.from_numpy(target_batch).float().to(self.device)\n","\n","        # Gather Q-values from network and replay memory actions\n","        Q_values = torch.gather(self.DQN_network(state_batch), dim=-1, index=action_batch.unsqueeze(-1)).squeeze(-1)\n","\n","        # Optimization step\n","        batch_loss = self.loss_function(Q_values, target_batch)\n","        batch_loss.backward()\n","        self.optimizer.step()\n","        batch_loss = batch_loss.item()\n","        self.DQN_network.eval()\n","        return batch_loss\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCR4YvyTl8Ga","executionInfo":{"status":"ok","timestamp":1605612313204,"user_tz":-60,"elapsed":634,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from copy import deepcopy\n","import random\n","\n","class DQN_agent(object):\n","    '''\n","    DQN agent\n","    '''\n","    def __init__(self,\n","                state_no,\n","                act_no,\n","                extra_action_version=0,\n","                replay_memory_capacity=20000,\n","                replay_memory_min_sample=1000,\n","                batch_size=32,\n","                training_period=1,\n","                discount_factor=0.99,\n","                hidden_layers=[64, 32],\n","                learning_rate=0.0001,\n","                epsilon_decay_steps=20000,\n","                update_target_dqn_period=1000, \n","                device=None):\n","\n","        '''\n","        Initialize the DQN agent\n","\n","        :param int state_no: Number of states\n","        :param int act_no: Number of actions\n","        :param int extra_action_version: Mode of choosing action during evaluation phase. Action with maximum value: 0, Raise action instead of Call if possible: 1, Raise action instead of Check if possible: 2, Raise action instead of Fold if possible: 3\n","        :param int replay_memory_capacity: Replay memory size\n","        :param int replay_memory_min_sample: Minimum number of samples in the replay memory during sampling\n","        :param int batch_size: Size of batches to sample from the replay memory\n","        :param int training_period: Train the network in every N steps\n","        :param float discount_factor: Discount factor (gamma) during training the agent\n","        :param list[int] hidden_layers: Dimensions of the hidden layers in the DQN network\n","        :param float learning_rate: The learning rate in the DQN network\n","        :param int epsilon_decay_steps: Number of steps to decay epsilon\n","        :param int update_target_dqn_period: Update target network in every N steps\n","        :param torch.device device: Usage CPU or GPU\n","        '''\n","        \n","        self.replay_memory_min_sample = replay_memory_min_sample\n","        self.update_target_dqn_period = update_target_dqn_period\n","        self.discount_factor = discount_factor\n","        self.epsilon_decay_steps = epsilon_decay_steps\n","        self.batch_size = batch_size\n","        self.act_no = act_no\n","        self.training_period = training_period\n","        self.extra_action_version = extra_action_version\n","\n","        # Torch device on which a torch.Tensor will be allocated\n","        if device is None:\n","            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        else:\n","            self.device = device\n","\n","        # Create the replay memory\n","        self.memory = ReplayMemory(replay_memory_capacity, batch_size)\n","\n","        # Initialize current timestep and current training timestep\n","        self.current_timestep, self.current_training_timestep = 0, 0\n","\n","        # Create array for the epsilon values during the epsilon decay \n","        self.epsilons = np.linspace(1.0, 0.1, epsilon_decay_steps)\n","\n","        # Create the policy and the target network\n","        self.policy_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n","        self.target_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n","\n","        # Set use_raw value for the RLCard environment\n","        self.use_raw = False\n","\n","    def store_and_train(self, transition):\n","        ''' \n","        Save transition into memory and train the agent based on the training period.\n","\n","        :param tuple transition: The transition tuple 'state', 'action', 'reward', 'next_state', 'done'\n","        \n","        '''\n","        (state, action, reward, next_state, done) = tuple(transition)\n","\n","        # Store transition in replay memory\n","        self.memory.push(state['obs'], action, reward, next_state['obs'], done)\n","        # Increment the number of timesteps\n","        self.current_timestep += 1\n","        # Train the agent if the replay memory has data already and agent reached the next training period\n","        time_between = self.current_timestep - self.replay_memory_min_sample\n","        if time_between>=0 and time_between%self.training_period == 0:\n","            self.train()\n","\n","    def discard_invalid_actions(self, action_probs, valid_actions):\n","        ''' \n","        Remove invalid actions and normalize the probabilities.\n","\n","        :param numpy.array[float] action_probs: Probabilities of all action\n","        :param list[int] valid_actions: Valid actions in the current state\n","        :return numpy.array[float] norm_valid_action_probs: Probabilities of valid actions\n","        '''\n","        # Initialize new array\n","        norm_valid_action_probs = np.zeros(action_probs.shape[0])\n","        # Add probability values of valid actions to the array\n","        norm_valid_action_probs[valid_actions] = action_probs[valid_actions]\n","        # Normalize probabilities\n","        norm_valid_action_probs[valid_actions] = 1 / len(valid_actions)\n","        return norm_valid_action_probs\n","\n","    def predict(self, state):\n","        ''' \n","        Predict the action probabilities.\n","\n","        :param numpy.array[float] state: Current state\n","        :return numpy.array[float] q_values: Array of Q values  \n","        '''\n","        epsilon = self.epsilons[min(self.current_timestep, self.epsilon_decay_steps-1)]\n","        actions = np.ones(self.act_no, dtype=float) * epsilon / self.act_no\n","        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state, 0))[0]\n","        best_action = np.argmax(q_values)\n","        actions[best_action] += (1.0 - epsilon)\n","        return actions\n","\n","    def step(self, state):\n","        ''' \n","        Define step function for the RLCard environment.\n","        Get the action for the current state for training purpose.\n","        If neccessary, remove invalid action pobabilities.\n","\n","        :param numpy.array state: The current state\n","        :return int action: The chosen action in the current state\n","        '''\n","        actions = self.predict(state['obs'])\n","        norm_valid_action_probs = self.discard_invalid_actions(actions, state['legal_actions'])\n","        action = np.random.choice(np.arange(len(actions)), p=norm_valid_action_probs)\n","        return action\n","\n","\n","    def eval_step(self, state):\n","        ''' \n","        Define eval_step function for the RLCard environment.\n","        Get the action for the evaluation purpose instead of training purpose.\n","\n","        :param numpy.array state: The current state\n","        :return int action: The chosen action in the current state\n","        '''\n","        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state['obs'], 0))[0]\n","        norm_valid_action_probs = self.discard_invalid_actions(np.exp(q_values), state['legal_actions'])\n","        # Check version of choosing action\n","        if self.extra_action_version == 1:\n","          # If Raise (1) is a valid action and the best action is Call (0)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==0:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        elif self.extra_action_version == 2:\n","          # If Raise (1) is a valid action and the best action is Check (3)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==3:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        elif self.extra_action_version == 3:\n","          # If Raise (1) is a valid action and the best action is Fold (2)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==2:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        else:\n","          best_action = np.argmax(norm_valid_action_probs)\n","        return best_action, norm_valid_action_probs\n","\n","    \n","    def train(self):\n","        ''' \n","        Train the agent.\n","\n","        return float loss: The loss of the current batch\n","        '''\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n","\n","        # Get best next action using the policy network\n","        q_values_next = self.policy_dqn.get_qvalue(next_state_batch)\n","        best_actions = np.argmax(q_values_next, axis=1)\n","\n","        # Calculate Q values from the target policy\n","        q_values_next_target = self.target_dqn.get_qvalue(next_state_batch)\n","        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n","\n","        # Update policy network\n","        state_batch = np.array(state_batch)\n","        loss = self.policy_dqn.update(state_batch, action_batch, target_batch)\n","\n","        # Update target network based on the target update period\n","        if self.current_training_timestep % self.update_target_dqn_period == 0:\n","            self.target_dqn = deepcopy(self.policy_dqn)\n","\n","        self.current_training_timestep += 1\n","\n","\n","    def get_state_dict(self):\n","        ''' \n","        Get the state dictionaries.\n","\n","        :return dict model_dict: Dictionaries containing the whole state of the policy and target modules\n","        '''\n","        model_dict = {'policy_network': self.policy_dqn.DQN_network.state_dict(), 'target_network': self.target_dqn.DQN_network.state_dict()}\n","        return model_dict\n","\n","    def load_networks(self, checkpoint):\n","        ''' \n","        Load network models.\n","\n","        :param dict checkpoint: Checkpoint of the policy and target networks\n","        '''\n","        self.policy_dqn.DQN_network.load_state_dict(checkpoint['policy_network'])\n","        self.target_dqn.DQN_network.load_state_dict(checkpoint['target_network'])"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZK0dotrUla77","executionInfo":{"status":"ok","timestamp":1605612328202,"user_tz":-60,"elapsed":13872,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"bf019234-5184-4973-82d4-97dbae28db6c","colab":{"base_uri":"https://localhost:8080/"}},"source":["import rlcard\n","from rlcard import models\n","from rlcard.agents import RandomAgent\n","from rlcard.utils import seeding, tournament\n","from rlcard.utils import Logger\n","import torch\n","import os\n","\n","# Create environments\n","env = rlcard.make('limit-holdem', config={'seed': 0})\n","eval_env = rlcard.make('limit-holdem', config={'seed': 0})\n","\n","# Set a global seed\n","seeding.create_seed(0)\n","\n","# Play agressive game based on the version of choosing actual action\n","# Action with maximum value: 0\n","# Raise action instead of Call if possible: 1\n","# Raise action instead of Check if possible: 2\n","# Raise action instead of Fold if possible: 3\n","extra_action_version=1\n","\n","# The paths for saving the logs and learning curves\n","log_dir = './experiments/limit_holdem_dqn_result/'\n","\n","# Create DQN agent\n","agent = DQN_agent(state_no=env.state_shape,\n","                  act_no=env.action_num, \n","                  replay_memory_min_sample=1000,\n","                  training_period=10,\n","                  hidden_layers=[128, 128],\n","                  device=torch.device('cpu'),\n","                  extra_action_version=extra_action_version)\n","\n","# Create random opponent agent\n","random_agent = RandomAgent(action_num=eval_env.action_num)\n","\n","# Add the agent to the environments\n","env.set_agents([agent, random_agent])\n","eval_env.set_agents([agent, random_agent])\n","\n","# Initialize logger\n","logger = Logger(log_dir)\n","\n","# Number of episodes, number of games during evaluation and evaluation in every N steps\n","episode_no, evaluate_games, evaluate_period = 1000, 100, 10\n","\n","for episode in range(episode_no):\n","    # Generate data from the environment\n","    trajectories, _ = env.run(is_training=True)\n","\n","    # Feed transitions into agent memory, and train the agent\n","    for ts in trajectories[0]:\n","        agent.store_and_train(ts)\n","\n","    # Evaluate the performance\n","    if episode % evaluate_period == 0:\n","        logger.log_performance(env.timestep, tournament(eval_env, evaluate_games)[0])\n","\n","# Close files in the logger\n","logger.close_files()\n","\n","# Save model\n","save_dir = 'models/dqn'\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","state_dict = agent.get_state_dict()\n","torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["\n","----------------------------------------\n","  timestep     |  2\n","  reward       |  2.46\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  28\n","  reward       |  3.015\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  47\n","  reward       |  2.98\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  71\n","  reward       |  3.525\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  102\n","  reward       |  3.095\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  128\n","  reward       |  3.065\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  156\n","  reward       |  2.855\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  186\n","  reward       |  3.05\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  216\n","  reward       |  2.665\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  256\n","  reward       |  2.04\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  273\n","  reward       |  2.97\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  310\n","  reward       |  2.62\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  351\n","  reward       |  2.28\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  383\n","  reward       |  3.07\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  404\n","  reward       |  3.135\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  422\n","  reward       |  3.035\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  444\n","  reward       |  2.77\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  491\n","  reward       |  2.745\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  527\n","  reward       |  2.015\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  554\n","  reward       |  2.58\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  583\n","  reward       |  2.38\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  611\n","  reward       |  1.69\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  647\n","  reward       |  2.695\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  685\n","  reward       |  2.695\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  709\n","  reward       |  2.39\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  732\n","  reward       |  3.445\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  766\n","  reward       |  3.3\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  796\n","  reward       |  3.45\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  816\n","  reward       |  3.02\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  853\n","  reward       |  3.205\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  893\n","  reward       |  3.03\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  918\n","  reward       |  2.915\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  959\n","  reward       |  2.255\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  977\n","  reward       |  2.89\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1003\n","  reward       |  2.81\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1041\n","  reward       |  3.23\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1065\n","  reward       |  2.26\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1094\n","  reward       |  2.655\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1140\n","  reward       |  2.995\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1163\n","  reward       |  4.2\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1200\n","  reward       |  3.22\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1225\n","  reward       |  4.17\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1247\n","  reward       |  2.66\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1278\n","  reward       |  2.21\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1321\n","  reward       |  3.015\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1352\n","  reward       |  2.51\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1384\n","  reward       |  1.935\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1408\n","  reward       |  2.615\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1439\n","  reward       |  2.87\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1470\n","  reward       |  2.42\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1513\n","  reward       |  3.94\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1539\n","  reward       |  2.09\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1569\n","  reward       |  3.215\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1612\n","  reward       |  2.75\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1653\n","  reward       |  2.84\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1694\n","  reward       |  2.65\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1731\n","  reward       |  3.15\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1763\n","  reward       |  2.34\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1785\n","  reward       |  3.275\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1809\n","  reward       |  2.975\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1826\n","  reward       |  2.42\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1855\n","  reward       |  3.195\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1898\n","  reward       |  2.84\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1917\n","  reward       |  2.88\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1950\n","  reward       |  3.505\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1998\n","  reward       |  1.75\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2030\n","  reward       |  2.43\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2053\n","  reward       |  2.41\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2084\n","  reward       |  2.925\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2110\n","  reward       |  2.585\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2146\n","  reward       |  2.51\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2167\n","  reward       |  2.58\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2202\n","  reward       |  2.665\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2234\n","  reward       |  3.02\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2264\n","  reward       |  2.715\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2298\n","  reward       |  2.515\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2339\n","  reward       |  3.155\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2361\n","  reward       |  3.17\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2405\n","  reward       |  3.39\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2433\n","  reward       |  1.92\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2465\n","  reward       |  3.075\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2491\n","  reward       |  2.65\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2521\n","  reward       |  2.93\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2555\n","  reward       |  3.215\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2588\n","  reward       |  2.905\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2603\n","  reward       |  2.68\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2629\n","  reward       |  2.885\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2657\n","  reward       |  3.035\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2688\n","  reward       |  3.12\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2711\n","  reward       |  3.7\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2744\n","  reward       |  3.895\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2783\n","  reward       |  2.795\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2815\n","  reward       |  3.035\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2827\n","  reward       |  2.775\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2846\n","  reward       |  3.67\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2871\n","  reward       |  2.985\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2896\n","  reward       |  2.475\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2922\n","  reward       |  3.12\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2955\n","  reward       |  2.775\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2976\n","  reward       |  2.275\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UcTuXaqCnq5B","executionInfo":{"status":"ok","timestamp":1605612341695,"user_tz":-60,"elapsed":865,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"c64b63da-3087-4427-d4a7-5d0ad059243b","colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["import os\n","import csv\n","from scipy.ndimage.filters import gaussian_filter1d\n","\n","csv_path = os.path.join(log_dir, 'performance.csv')\n","save_path = log_dir\n","\n","\n","def plot(algorithm):\n","    ''' \n","    Read data from csv file and plot the results\n","    '''\n","    import matplotlib.pyplot as plt\n","    with open(csv_path) as csvfile:\n","        print(csv_path)\n","        reader = csv.DictReader(csvfile)\n","        xs = []\n","        ys = []\n","        for row in reader:\n","            xs.append(int(row['timestep']))\n","            ys.append(float(row['reward']))\n","        fig, ax = plt.subplots()\n","        \n","        # Calculate the trendline\n","        z = np.polyfit(xs, ys, 10)\n","        p = np.poly1d(z)\n","        ax.plot(xs, p(xs))\n","\n","        #ax.plot(xs[:-(N-1)], moving_aves, label=algorithm)\n","        ax.set(xlabel='timestep', ylabel='reward', title=algorithm)\n","        ax.legend()\n","        ax.grid()\n","\n","title = 'Limit Holdem DQN action version: ' + str(extra_action_version)\n","# Plot the learning curve\n","plot(title)\n"],"execution_count":73,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"stream","text":["./experiments/limit_holdem_dqn_result/performance.csv\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"PIKTbdgkcC8o"},"source":[""],"execution_count":null,"outputs":[]}]}