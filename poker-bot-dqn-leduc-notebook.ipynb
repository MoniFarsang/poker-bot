{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poker-bot-dqn-leduc-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRJsfnUrmS_H",
        "outputId": "f76f3545-5204-4918-b13b-a9907d665484"
      },
      "source": [
        "!pip install git+https://github.com/datamllab/rlcard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/datamllab/rlcard\n",
            "  Cloning https://github.com/datamllab/rlcard to /tmp/pip-req-build-_wcy6hwg\n",
            "  Running command git clone -q https://github.com/datamllab/rlcard /tmp/pip-req-build-_wcy6hwg\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.18.5)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (3.2.2)\n",
            "Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (7.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (20.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard==0.2.6) (1.15.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-0.2.6-cp36-none-any.whl size=6785384 sha256=286ec2656637a1d4b4ded15521b24af4ab2c3843ab9807e1f5829fa5368c1c8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-048enisy/wheels/b3/e1/32/6535ad7ff9142e4c031af97e237e4df3e4ab14e86194738ac4\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard\n",
            "Successfully installed rlcard-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9-pPwFQmjxb",
        "outputId": "40c16d44-1f7d-4936-dbe2-3b9cf99bf953"
      },
      "source": [
        "%tensorflow_version 1.x # for using tensorflow.contrib\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x # for using tensorflow.contrib`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DjHYr9NmBE8"
      },
      "source": [
        "from collections import namedtuple\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    ''' \n",
        "    Replay memory for saving transitions\n",
        "    '''\n",
        "    def __init__(self, capacity, batch_size):\n",
        "        ''' \n",
        "        Initialize ReplayMemory\n",
        "\n",
        "        :param int capacity: the size of the memory buffer\n",
        "        :param int batch_size: the size of the batches\n",
        "        '''\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        '''\n",
        "        Save a transition into memory\n",
        "        '''\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Choose random sample from the memory with size of the batch size\n",
        "        '''\n",
        "        samples = random.sample(self.memory, batch_size)\n",
        "        return map(np.array, zip(*samples))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJOP-_FYlz_X"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DQN_network(object):\n",
        "    '''\n",
        "    Deep Q-Network\n",
        "    '''\n",
        "\n",
        "    def __init__(self, state_no=36, act_no=4, hidden_layers=[64, 32], learning_rate=0.001, device=None):\n",
        "        ''' \n",
        "        Initilalize the DQN_network object.\n",
        "\n",
        "        :param act_no (int): Number of actions (4 in Leduc Hold'em)\n",
        "        :param state_no (list): Size of the state space (36 in Leduc Hold'em)\n",
        "        :param hidden_layers (list): Dimension of the hidden layers\n",
        "        :param device (torch.device): Usage CPU or GPU\n",
        "        '''\n",
        "        self.state_no = state_no\n",
        "        self.act_no = act_no\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate=learning_rate\n",
        "        self.device = device\n",
        "\n",
        "        # DQN network based on the layers\n",
        "        layers = self.state_no + self.hidden_layers\n",
        "        DQN_network = [nn.Flatten()]\n",
        "        DQN_network.append(nn.BatchNorm1d(layers[0]))\n",
        "        for i in range(len(layers)-1):\n",
        "            DQN_network.append(nn.Linear(layers[i], layers[i+1], bias=True))\n",
        "            DQN_network.append(nn.Tanh())\n",
        "        DQN_network.append(nn.Linear(layers[-1], self.act_no, bias=True))\n",
        "        DQN_network = nn.Sequential(*DQN_network)\n",
        "\n",
        "        DQN_network = DQN_network.to(self.device)\n",
        "        self.DQN_network = DQN_network\n",
        "        self.DQN_network.eval()\n",
        "\n",
        "        # Initialize weights in the network\n",
        "        for p in self.DQN_network.parameters():\n",
        "            if len(p.data.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "        # Define loss function\n",
        "        self.loss_function = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Define optimizer\n",
        "        #self.optimizer =  torch.optim.Adam(self.DQN_network.parameters(), lr=self.learning_rate)\n",
        "        self.optimizer = torch.optim.RMSprop(self.DQN_network.parameters())\n",
        "\n",
        "\n",
        "    def get_qvalue(self, next_state_batch):\n",
        "        ''' \n",
        "        Get Q-values for the batch of the next states.\n",
        "        It does not use gradient calculation.\n",
        "\n",
        "        :param np.ndarray next_state_batch: Batch of the next states\n",
        "        :return np.ndarray Q_values: The estimated Q-values\n",
        "        '''\n",
        "        # Disable gradient calculation\n",
        "        with torch.no_grad():\n",
        "            # Create torch tensor\n",
        "            next_state_batch = torch.from_numpy(next_state_batch).float().to(self.device)\n",
        "            # Get Q values\n",
        "            Q_values = self.DQN_network(next_state_batch).cpu().numpy()\n",
        "        return Q_values\n",
        "\n",
        "    def update(self, state_batch, action_batch, target_batch):\n",
        "        ''' \n",
        "        Update the policy network\n",
        "\n",
        "        :param np.ndarray state_batch: Batch of states from replay memory\n",
        "        :param np.ndarray action_batch: Batch of actions from replay memory\n",
        "        :param np.ndarray target_batch: Batch of Q-values from the target policy, it used during the optimization step\n",
        "        :return float batch_loss: The calculated loss on the batch       \n",
        "        '''\n",
        "        # Set the gradients to zero\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Set the network in training mode\n",
        "        self.DQN_network.train()\n",
        "\n",
        "        # Create torch tensors\n",
        "        state_batch = torch.from_numpy(state_batch).float().to(self.device)\n",
        "        action_batch = torch.from_numpy(action_batch).long().to(self.device)\n",
        "        target_batch = torch.from_numpy(target_batch).float().to(self.device)\n",
        "\n",
        "        # Gather Q-values from network and replay memory actions\n",
        "        Q_values = torch.gather(self.DQN_network(state_batch), dim=-1, index=action_batch.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optimization step\n",
        "        batch_loss = self.loss_function(Q_values, target_batch)\n",
        "        batch_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        batch_loss = batch_loss.item()\n",
        "        self.DQN_network.eval()\n",
        "        return batch_loss\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCR4YvyTl8Ga"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "class DQN_agent(object):\n",
        "    '''\n",
        "    DQN agent\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                state_no,\n",
        "                act_no,\n",
        "                extra_action_version=0,\n",
        "                replay_memory_capacity=20000,\n",
        "                replay_memory_min_sample=1000,\n",
        "                batch_size=32,\n",
        "                training_period=1,\n",
        "                discount_factor=0.99,\n",
        "                hidden_layers=[64, 32],\n",
        "                learning_rate=0.0001,\n",
        "                epsilon_decay_steps=20000,\n",
        "                update_target_dqn_period=1000, \n",
        "                device=None):\n",
        "\n",
        "        '''\n",
        "        Initialize the DQN agent\n",
        "\n",
        "        :param int state_no: Number of states\n",
        "        :param int act_no: Number of actions\n",
        "        :param int extra_action_version: Mode of choosing action during evaluation phase. Action with maximum value: 0, Raise action instead of Call if possible: 1, Raise action instead of Check if possible: 2, Raise action instead of Fold if possible: 3\n",
        "        :param int replay_memory_capacity: Replay memory size\n",
        "        :param int replay_memory_min_sample: Minimum number of samples in the replay memory during sampling\n",
        "        :param int batch_size: Size of batches to sample from the replay memory\n",
        "        :param int training_period: Train the network in every N steps\n",
        "        :param float discount_factor: Discount factor (gamma) during training the agent\n",
        "        :param list[int] hidden_layers: Dimensions of the hidden layers in the DQN network\n",
        "        :param float learning_rate: The learning rate in the DQN network\n",
        "        :param int epsilon_decay_steps: Number of steps to decay epsilon\n",
        "        :param int update_target_dqn_period: Update target network in every N steps\n",
        "        :param torch.device device: Usage CPU or GPU\n",
        "        '''\n",
        "        \n",
        "        self.replay_memory_min_sample = replay_memory_min_sample\n",
        "        self.update_target_dqn_period = update_target_dqn_period\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.act_no = act_no\n",
        "        self.training_period = training_period\n",
        "        self.extra_action_version = extra_action_version\n",
        "\n",
        "        # Torch device on which a torch.Tensor will be allocated\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        # Create the replay memory\n",
        "        self.memory = ReplayMemory(replay_memory_capacity, batch_size)\n",
        "\n",
        "        # Initialize current timestep and current training timestep\n",
        "        self.current_timestep, self.current_training_timestep = 0, 0\n",
        "\n",
        "        # Create array for the epsilon values during the epsilon decay \n",
        "        self.epsilons = np.linspace(1.0, 0.1, epsilon_decay_steps)\n",
        "\n",
        "        # Create the policy and the target network\n",
        "        self.policy_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n",
        "        self.target_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n",
        "\n",
        "        # Set use_raw value for the RLCard environment\n",
        "        self.use_raw = False\n",
        "\n",
        "    def store_and_train(self, transition):\n",
        "        ''' \n",
        "        Save transition into memory and train the agent based on the training period.\n",
        "\n",
        "        :param tuple transition: The transition tuple 'state', 'action', 'reward', 'next_state', 'done'\n",
        "        \n",
        "        '''\n",
        "        (state, action, reward, next_state, done) = tuple(transition)\n",
        "\n",
        "        # Store transition in replay memory\n",
        "        self.memory.push(state['obs'], action, reward, next_state['obs'], done)\n",
        "        # Increment the number of timesteps\n",
        "        self.current_timestep += 1\n",
        "        # Train the agent if the replay memory has data already and agent reached the next training period\n",
        "        time_between = self.current_timestep - self.replay_memory_min_sample\n",
        "        if time_between>=0 and time_between%self.training_period == 0:\n",
        "            self.train()\n",
        "\n",
        "    def discard_invalid_actions(self, action_probs, valid_actions):\n",
        "        ''' \n",
        "        Remove invalid actions and normalize the probabilities.\n",
        "\n",
        "        :param numpy.array[float] action_probs: Probabilities of all action\n",
        "        :param list[int] valid_actions: Valid actions in the current state\n",
        "        :return numpy.array[float] norm_valid_action_probs: Probabilities of valid actions\n",
        "        '''\n",
        "        # Initialize new array\n",
        "        norm_valid_action_probs = np.zeros(action_probs.shape[0])\n",
        "        # Add probability values of valid actions to the array\n",
        "        norm_valid_action_probs[valid_actions] = action_probs[valid_actions]\n",
        "        # Normalize probabilities\n",
        "        norm_valid_action_probs[valid_actions] = 1 / len(valid_actions)\n",
        "        return norm_valid_action_probs\n",
        "\n",
        "    def predict(self, state):\n",
        "        ''' \n",
        "        Predict the action probabilities.\n",
        "\n",
        "        :param numpy.array[float] state: Current state\n",
        "        :return numpy.array[float] q_values: Array of Q values  \n",
        "        '''\n",
        "        epsilon = self.epsilons[min(self.current_timestep, self.epsilon_decay_steps-1)]\n",
        "        actions = np.ones(self.act_no, dtype=float) * epsilon / self.act_no\n",
        "        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        actions[best_action] += (1.0 - epsilon)\n",
        "        return actions\n",
        "\n",
        "    def step(self, state):\n",
        "        ''' \n",
        "        Define step function for the RLCard environment.\n",
        "        Get the action for the current state for training purpose.\n",
        "        If neccessary, remove invalid action pobabilities.\n",
        "\n",
        "        :param numpy.array state: The current state\n",
        "        :return int action: The chosen action in the current state\n",
        "        '''\n",
        "        actions = self.predict(state['obs'])\n",
        "        norm_valid_action_probs = self.discard_invalid_actions(actions, state['legal_actions'])\n",
        "        action = np.random.choice(np.arange(len(actions)), p=norm_valid_action_probs)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        ''' \n",
        "        Define eval_step function for the RLCard environment.\n",
        "        Get the action for the evaluation purpose instead of training purpose.\n",
        "\n",
        "        :param numpy.array state: The current state\n",
        "        :return int action: The chosen action in the current state\n",
        "        '''\n",
        "        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state['obs'], 0))[0]\n",
        "        norm_valid_action_probs = self.discard_invalid_actions(np.exp(q_values), state['legal_actions'])\n",
        "        # Check version of choosing action\n",
        "        if self.extra_action_version == 1:\n",
        "          # If Raise (1) is a valid action and the best action is Call (0)\n",
        "          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==0:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(norm_valid_action_probs)\n",
        "        elif self.extra_action_version == 2:\n",
        "          # If Raise (1) is a valid action and the best action is Check (3)\n",
        "          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==3:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(norm_valid_action_probs)\n",
        "        elif self.extra_action_version == 3:\n",
        "          # If Raise (1) is a valid action and the best action is Fold (2)\n",
        "          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==2:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(norm_valid_action_probs)\n",
        "        else:\n",
        "          best_action = np.argmax(norm_valid_action_probs)\n",
        "        return best_action, norm_valid_action_probs\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "        ''' \n",
        "        Train the agent.\n",
        "\n",
        "        return float loss: The loss of the current batch\n",
        "        '''\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Get best next action using the policy network\n",
        "        q_values_next = self.policy_dqn.get_qvalue(next_state_batch)\n",
        "        best_actions = np.argmax(q_values_next, axis=1)\n",
        "\n",
        "        # Calculate Q values from the target policy\n",
        "        q_values_next_target = self.target_dqn.get_qvalue(next_state_batch)\n",
        "        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n",
        "\n",
        "        # Update policy network\n",
        "        state_batch = np.array(state_batch)\n",
        "        loss = self.policy_dqn.update(state_batch, action_batch, target_batch)\n",
        "\n",
        "        # Update target network based on the target update period\n",
        "        if self.current_training_timestep % self.update_target_dqn_period == 0:\n",
        "            self.target_dqn = deepcopy(self.policy_dqn)\n",
        "\n",
        "        self.current_training_timestep += 1\n",
        "\n",
        "\n",
        "    def get_state_dict(self):\n",
        "        ''' \n",
        "        Get the state dictionaries.\n",
        "\n",
        "        :return dict model_dict: Dictionaries containing the whole state of the policy and target modules\n",
        "        '''\n",
        "        model_dict = {'policy_network': self.policy_dqn.DQN_network.state_dict(), 'target_network': self.target_dqn.DQN_network.state_dict()}\n",
        "        return model_dict\n",
        "\n",
        "    def load_networks(self, checkpoint):\n",
        "        ''' \n",
        "        Load network models.\n",
        "\n",
        "        :param dict checkpoint: Checkpoint of the policy and target networks\n",
        "        '''\n",
        "        self.policy_dqn.DQN_network.load_state_dict(checkpoint['policy_network'])\n",
        "        self.target_dqn.DQN_network.load_state_dict(checkpoint['target_network'])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YajsvaIsw2W4"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(logdir, title):\n",
        "    ''' \n",
        "    Read data from csv file and plot the results\n",
        "\n",
        "    :param string logdir: Logging directory\n",
        "    :param string title: Title of the plot\n",
        "    '''\n",
        "    csv_path = os.path.join(log_dir, 'performance.csv')\n",
        "    save_path = log_dir\n",
        "\n",
        "    with open(csv_path) as csvfile:\n",
        "        print(csv_path)\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        xs = [0]\n",
        "        ys = [0]\n",
        "        for row in reader:\n",
        "            xs.append(int(row['timestep']))\n",
        "            ys.append(float(row['reward']))\n",
        "        plt.plot(xs, ys)\n",
        "        plt.xlabel('timestep')\n",
        "        plt.ylabel('reward')\n",
        "        plt.title(title)\n",
        "        plt.legend(['DQN'])\n",
        "        plt.ylim(min(-0.5, min(ys)), max(ys)+0.5)\n",
        "        plt.grid()\n",
        "        plt.savefig(save_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK0dotrUla77",
        "outputId": "38f3782e-77e4-41c2-9cc3-e06d1ea3c9c4"
      },
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import seeding, tournament\n",
        "from rlcard.utils import Logger\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Create environments\n",
        "env = rlcard.make('leduc-holdem', config={'seed': 0})\n",
        "eval_env = rlcard.make('leduc-holdem', config={'seed': 0})\n",
        "\n",
        "# Set a global seed\n",
        "seeding.create_seed(0)\n",
        "\n",
        "# Play agressive game based on the version of choosing actual action\n",
        "# Action with maximum value: 0\n",
        "# Raise action instead of Call if possible: 1\n",
        "# Raise action instead of Check if possible: 2\n",
        "# Raise action instead of Fold if possible: 3\n",
        "extra_action_version=1\n",
        "\n",
        "# Opponent agent\n",
        "# Random agent: 0\n",
        "# Pretrained agent with nfsp: 1\n",
        "opponent_agent_version_train=1\n",
        "opponent_agent_version_eval=0\n",
        "\n",
        "# The paths for saving the logs and learning curves\n",
        "log_dir = './experiments/leduc_holdem_dqn_result/'\n",
        "\n",
        "# Create DQN agent\n",
        "agent = DQN_agent(state_no=env.state_shape,\n",
        "                  act_no=env.action_num, \n",
        "                  replay_memory_min_sample=1000,\n",
        "                  training_period=10,\n",
        "                  hidden_layers=[128, 128],\n",
        "                  device=torch.device('cpu'),\n",
        "                  extra_action_version=extra_action_version)\n",
        "\n",
        "# Create opponent agent for training\n",
        "if opponent_agent_version_train == 1:\n",
        "  # Create a pre-trained NFSP agent\n",
        "  opponent_agent_train = models.load('leduc-holdem-nfsp').agents[0]\n",
        "else:\n",
        "  # Create a random agent\n",
        "  opponent_agent_train = RandomAgent(action_num=eval_env.action_num)\n",
        "\n",
        "# Create opponent agent for evaluation\n",
        "if opponent_agent_version_eval == 1:\n",
        "  # Create a pre-trained NFSP agent\n",
        "  opponent_agent_eval = models.load('leduc-holdem-nfsp').agents[0]\n",
        "else:\n",
        "  # Create a random agent\n",
        "  opponent_agent_eval = RandomAgent(action_num=eval_env.action_num)\n",
        "\n",
        "# Add the agent to the environments\n",
        "env.set_agents([agent, opponent_agent_train])\n",
        "eval_env.set_agents([agent, opponent_agent_eval])\n",
        "\n",
        "# Initialize logger\n",
        "logger = Logger(log_dir)\n",
        "\n",
        "# Number of episodes, number of games during evaluation and evaluation in every N steps\n",
        "episode_no, evaluate_games, evaluate_period = 1000, 100, 10\n",
        "\n",
        "for episode in range(episode_no):\n",
        "    # Generate data from the environment\n",
        "    trajectories, _ = env.run(is_training=True)\n",
        "\n",
        "    # Feed transitions into agent memory, and train the agent\n",
        "    for ts in trajectories[0]:\n",
        "        agent.store_and_train(ts)\n",
        "\n",
        "    # Evaluate the performance\n",
        "    if episode % evaluate_period == 0:\n",
        "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_games)[0])\n",
        "\n",
        "# Close files in the logger\n",
        "logger.close_files()\n",
        "\n",
        "# Save model\n",
        "save_dir = 'models/dqn'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "state_dict = agent.get_state_dict()\n",
        "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained_models.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/nfsp_agent.py:114: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:256: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:267: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:280: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:244: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:247: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained_models.py:40: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained/leduc_holdem_nfsp/model\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2\n",
            "  reward       |  1.045\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  35\n",
            "  reward       |  0.985\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  72\n",
            "  reward       |  0.985\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  109\n",
            "  reward       |  1.51\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  137\n",
            "  reward       |  1.215\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  177\n",
            "  reward       |  1.535\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  213\n",
            "  reward       |  1.245\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  253\n",
            "  reward       |  1.235\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  291\n",
            "  reward       |  0.995\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  324\n",
            "  reward       |  1.42\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  353\n",
            "  reward       |  1.085\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  388\n",
            "  reward       |  1.17\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  435\n",
            "  reward       |  1.055\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  471\n",
            "  reward       |  1.43\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  504\n",
            "  reward       |  0.7\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  543\n",
            "  reward       |  1.27\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  577\n",
            "  reward       |  1.185\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  614\n",
            "  reward       |  1.66\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  642\n",
            "  reward       |  1.145\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  680\n",
            "  reward       |  0.965\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  714\n",
            "  reward       |  1.53\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  753\n",
            "  reward       |  1.325\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  788\n",
            "  reward       |  1.655\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  819\n",
            "  reward       |  1.06\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  862\n",
            "  reward       |  1.305\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  896\n",
            "  reward       |  0.955\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  932\n",
            "  reward       |  1.125\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  972\n",
            "  reward       |  0.89\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1007\n",
            "  reward       |  1.185\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1041\n",
            "  reward       |  1.135\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1065\n",
            "  reward       |  1.235\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1102\n",
            "  reward       |  1.315\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1132\n",
            "  reward       |  1.06\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1173\n",
            "  reward       |  1.065\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1205\n",
            "  reward       |  1.47\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1237\n",
            "  reward       |  1.215\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1271\n",
            "  reward       |  0.9\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1299\n",
            "  reward       |  1.46\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1330\n",
            "  reward       |  1.12\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1369\n",
            "  reward       |  1.34\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1410\n",
            "  reward       |  1.22\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1437\n",
            "  reward       |  1.37\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1467\n",
            "  reward       |  0.785\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1502\n",
            "  reward       |  1.25\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1534\n",
            "  reward       |  0.98\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1562\n",
            "  reward       |  0.825\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1596\n",
            "  reward       |  1.105\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1629\n",
            "  reward       |  1.085\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1656\n",
            "  reward       |  1.66\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1687\n",
            "  reward       |  0.775\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1718\n",
            "  reward       |  1.335\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1755\n",
            "  reward       |  1.2\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1788\n",
            "  reward       |  0.985\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1818\n",
            "  reward       |  1.035\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1848\n",
            "  reward       |  1.02\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1887\n",
            "  reward       |  1.38\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1920\n",
            "  reward       |  1.07\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1954\n",
            "  reward       |  1.245\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  1988\n",
            "  reward       |  0.855\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2028\n",
            "  reward       |  1.145\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2064\n",
            "  reward       |  1.22\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2100\n",
            "  reward       |  0.715\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2125\n",
            "  reward       |  1.54\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2152\n",
            "  reward       |  1.21\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2181\n",
            "  reward       |  1.105\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2207\n",
            "  reward       |  1.07\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2232\n",
            "  reward       |  0.91\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2270\n",
            "  reward       |  1.13\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2304\n",
            "  reward       |  1.0\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2340\n",
            "  reward       |  1.145\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2368\n",
            "  reward       |  1.28\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2403\n",
            "  reward       |  0.74\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2429\n",
            "  reward       |  1.485\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2467\n",
            "  reward       |  1.005\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2499\n",
            "  reward       |  0.98\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2532\n",
            "  reward       |  1.4\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2561\n",
            "  reward       |  1.03\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2605\n",
            "  reward       |  1.285\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2649\n",
            "  reward       |  0.68\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2687\n",
            "  reward       |  1.215\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2722\n",
            "  reward       |  1.39\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2759\n",
            "  reward       |  1.03\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2782\n",
            "  reward       |  0.995\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2813\n",
            "  reward       |  1.23\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2849\n",
            "  reward       |  1.28\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2890\n",
            "  reward       |  0.955\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2919\n",
            "  reward       |  1.03\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2949\n",
            "  reward       |  1.195\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  2986\n",
            "  reward       |  1.575\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3008\n",
            "  reward       |  0.905\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3036\n",
            "  reward       |  1.275\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3068\n",
            "  reward       |  1.14\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3092\n",
            "  reward       |  0.895\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3126\n",
            "  reward       |  1.115\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3159\n",
            "  reward       |  1.04\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3196\n",
            "  reward       |  1.725\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3226\n",
            "  reward       |  0.63\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3262\n",
            "  reward       |  0.84\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3300\n",
            "  reward       |  1.51\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "  timestep     |  3337\n",
            "  reward       |  1.205\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "UcTuXaqCnq5B",
        "outputId": "d7892ee3-56f5-472a-8bac-ecd05b450c2e"
      },
      "source": [
        "# Plot the learning curve\n",
        "title = 'Leduc Holdem DQN action version: ' + str(extra_action_version) + ', agent training: ' + str(opponent_agent_version_train) + ', agent eval: ' + str(opponent_agent_version_eval)\n",
        "plot(log_dir, title)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./experiments/leduc_holdem_dqn_result/performance.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-LgUO6Wh20AV",
        "outputId": "f4337d3c-ed93-40da-b7b6-df56972d10f0"
      },
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import LeducholdemHumanAgent as HumanAgent\n",
        "from rlcard.utils import print_card\n",
        "\n",
        "# Make environment\n",
        "# Set 'record_action' to True because we need it to print results\n",
        "env = rlcard.make('leduc-holdem', config={'record_action': True})\n",
        "human_agent = HumanAgent(env.action_num)\n",
        "env.set_agents([human_agent, agent])\n",
        "\n",
        "print(\">> Leduc Hold'em pre-trained model\")\n",
        "\n",
        "while (True):\n",
        "    print(\">> Start a new game\")\n",
        "\n",
        "    trajectories, payoffs = env.run(is_training=False)\n",
        "    # If the human does not take the final action, we need to\n",
        "    # print other players action\n",
        "    final_state = trajectories[0][-1][-2]\n",
        "    action_record = final_state['action_record']\n",
        "    state = final_state['raw_obs']\n",
        "    _action_list = []\n",
        "    for i in range(1, len(action_record)+1):\n",
        "        if action_record[-i][0] == state['current_player']:\n",
        "            break\n",
        "        _action_list.insert(0, action_record[-i])\n",
        "    for pair in _action_list:\n",
        "        print('>> Player', pair[0], 'chooses', pair[1])\n",
        "\n",
        "    # Let's take a look at what the agent card is\n",
        "    print('===============     CFR Agent    ===============')\n",
        "    print_card(env.get_perfect_information()['hand_cards'][1])\n",
        "\n",
        "    print('===============     Result     ===============')\n",
        "    if payoffs[0] > 0:\n",
        "        print('You win {} chips!'.format(payoffs[0]))\n",
        "    elif payoffs[0] == 0:\n",
        "        print('It is a tie.')\n",
        "    else:\n",
        "        print('You lose {} chips!'.format(-payoffs[0]))\n",
        "    print('')\n",
        "\n",
        "    input(\"Press any key to continue...\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Leduc Hold'em pre-trained model\n",
            ">> Start a new game\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++\n",
            "Agent 1: ++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 1\n",
            ">> Player 1 chooses call\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│K        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        K│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++++++\n",
            "Agent 1: ++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: raise, 1: fold, 2: check\n",
            "\n",
            ">> You choose action (integer): 2\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│K        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        K│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++++++\n",
            "Agent 1: ++++++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 0\n",
            ">> Player 0 chooses call\n",
            "===============     CFR Agent    ===============\n",
            "┌─────────┐\n",
            "│K        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        K│\n",
            "└─────────┘\n",
            "===============     Result     ===============\n",
            "You lose 5.0 chips!\n",
            "\n",
            "Press any key to continue...\n",
            ">> Start a new game\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   +\n",
            "Agent 1: ++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 0\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++\n",
            "Agent 1: ++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 0\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++++\n",
            "Agent 1: ++++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 1\n",
            ">> Player 1 chooses call\n",
            "===============     CFR Agent    ===============\n",
            "┌─────────┐\n",
            "│K        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        K│\n",
            "└─────────┘\n",
            "===============     Result     ===============\n",
            "You win 6.0 chips!\n",
            "\n",
            "Press any key to continue...\n",
            ">> Start a new game\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   +\n",
            "Agent 1: ++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 1\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++++\n",
            "Agent 1: ++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: fold\n",
            "\n",
            ">> You choose action (integer): 0\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│K        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        K│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++++++\n",
            "Agent 1: ++++++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 2\n",
            ">> Player 0 chooses fold\n",
            "===============     CFR Agent    ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============     Result     ===============\n",
            "You lose 3.0 chips!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ec92a7a1a8ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Press any key to continue...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}