{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"poker-bot-dqn-leduc-notebook.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPwelJ7GXp5vlz/luddIL1m"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aRJsfnUrmS_H","executionInfo":{"status":"ok","timestamp":1605605406639,"user_tz":-60,"elapsed":8447,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"e135408f-45f3-47ee-bb78-95bfb5981ef3","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install git+https://github.com/datamllab/rlcard"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/datamllab/rlcard\n","  Cloning https://github.com/datamllab/rlcard to /tmp/pip-req-build-rt9cjlbt\n","  Running command git clone -q https://github.com/datamllab/rlcard /tmp/pip-req-build-rt9cjlbt\n","Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.18.5)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (3.2.2)\n","Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (7.0.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from rlcard==0.2.6) (20.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->rlcard==0.2.6) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->rlcard==0.2.6) (1.15.0)\n","Building wheels for collected packages: rlcard\n","  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rlcard: filename=rlcard-0.2.6-cp36-none-any.whl size=6785365 sha256=41062c71f56c6d2c21e75b3a7f02e0aa4f4b8b4d36a0110b32d67ef79b912f85\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kj6mf45b/wheels/b3/e1/32/6535ad7ff9142e4c031af97e237e4df3e4ab14e86194738ac4\n","Successfully built rlcard\n","Installing collected packages: rlcard\n","Successfully installed rlcard-0.2.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V9-pPwFQmjxb","executionInfo":{"status":"ok","timestamp":1605605456180,"user_tz":-60,"elapsed":4981,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"697a396a-9249-480e-a8a4-db7ca10e2eb2","colab":{"base_uri":"https://localhost:8080/"}},"source":["%tensorflow_version 1.x # for using tensorflow.contrib\n","import tensorflow as tf"],"execution_count":2,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x # for using tensorflow.contrib`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_DjHYr9NmBE8","executionInfo":{"status":"ok","timestamp":1605605456183,"user_tz":-60,"elapsed":3787,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["from collections import namedtuple\n","import random\n","import numpy as np\n","\n","Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n","\n","class ReplayMemory(object):\n","    ''' \n","    Replay memory for saving transitions\n","    '''\n","    def __init__(self, capacity, batch_size):\n","        ''' \n","        Initialize ReplayMemory\n","\n","        :param int capacity: the size of the memory buffer\n","        :param int batch_size: the size of the batches\n","        '''\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        '''\n","        Save a transition into memory\n","        '''\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        self.position = (self.position + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        '''\n","        Choose random sample from the memory with size of the batch size\n","        '''\n","        samples = random.sample(self.memory, batch_size)\n","        return map(np.array, zip(*samples))\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJOP-_FYlz_X","executionInfo":{"status":"ok","timestamp":1605605464900,"user_tz":-60,"elapsed":3155,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","class DQN_network(object):\n","    '''\n","    Deep Q-Network\n","    '''\n","\n","    def __init__(self, state_no=36, act_no=4, hidden_layers=[64, 32], learning_rate=0.001, device=None):\n","        ''' \n","        Initilalize the DQN_network object.\n","\n","        :param act_no (int): Number of actions (4 in Leduc Hold'em)\n","        :param state_no (list): Size of the state space (36 in Leduc Hold'em)\n","        :param hidden_layers (list): Dimension of the hidden layers\n","        :param device (torch.device): Usage CPU or GPU\n","        '''\n","        self.state_no = state_no\n","        self.act_no = act_no\n","        self.hidden_layers = hidden_layers\n","        self.learning_rate=learning_rate\n","        self.device = device\n","\n","        # DQN network based on the layers\n","        layers = self.state_no + self.hidden_layers\n","        DQN_network = [nn.Flatten()]\n","        DQN_network.append(nn.BatchNorm1d(layers[0]))\n","        for i in range(len(layers)-1):\n","            DQN_network.append(nn.Linear(layers[i], layers[i+1], bias=True))\n","            DQN_network.append(nn.Tanh())\n","        DQN_network.append(nn.Linear(layers[-1], self.act_no, bias=True))\n","        DQN_network = nn.Sequential(*DQN_network)\n","\n","        DQN_network = DQN_network.to(self.device)\n","        self.DQN_network = DQN_network\n","        self.DQN_network.eval()\n","\n","        # Initialize weights in the network\n","        for p in self.DQN_network.parameters():\n","            if len(p.data.shape) > 1:\n","                nn.init.xavier_uniform_(p.data)\n","\n","        # Define loss function\n","        self.loss_function = nn.MSELoss(reduction='mean')\n","\n","        # Define optimizer\n","        #self.optimizer =  torch.optim.Adam(self.DQN_network.parameters(), lr=self.learning_rate)\n","        self.optimizer = torch.optim.RMSprop(self.DQN_network.parameters())\n","\n","\n","    def get_qvalue(self, next_state_batch):\n","        ''' \n","        Get Q-values for the batch of the next states.\n","        It does not use gradient calculation.\n","\n","        :param np.ndarray next_state_batch: Batch of the next states\n","        :return np.ndarray Q_values: The estimated Q-values\n","        '''\n","        # Disable gradient calculation\n","        with torch.no_grad():\n","            # Create torch tensor\n","            next_state_batch = torch.from_numpy(next_state_batch).float().to(self.device)\n","            # Get Q values\n","            Q_values = self.DQN_network(next_state_batch).cpu().numpy()\n","        return Q_values\n","\n","    def update(self, state_batch, action_batch, target_batch):\n","        ''' \n","        Update the policy network\n","\n","        :param np.ndarray state_batch: Batch of states from replay memory\n","        :param np.ndarray action_batch: Batch of actions from replay memory\n","        :param np.ndarray target_batch: Batch of Q-values from the target policy, it used during the optimization step\n","        :return float batch_loss: The calculated loss on the batch       \n","        '''\n","        # Set the gradients to zero\n","        self.optimizer.zero_grad()\n","\n","        # Set the network in training mode\n","        self.DQN_network.train()\n","\n","        # Create torch tensors\n","        state_batch = torch.from_numpy(state_batch).float().to(self.device)\n","        action_batch = torch.from_numpy(action_batch).long().to(self.device)\n","        target_batch = torch.from_numpy(target_batch).float().to(self.device)\n","\n","        # Gather Q-values from network and replay memory actions\n","        Q_values = torch.gather(self.DQN_network(state_batch), dim=-1, index=action_batch.unsqueeze(-1)).squeeze(-1)\n","\n","        # Optimization step\n","        batch_loss = self.loss_function(Q_values, target_batch)\n","        batch_loss.backward()\n","        self.optimizer.step()\n","        batch_loss = batch_loss.item()\n","        self.DQN_network.eval()\n","        return batch_loss\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCR4YvyTl8Ga","executionInfo":{"status":"ok","timestamp":1605609930568,"user_tz":-60,"elapsed":903,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from copy import deepcopy\n","import random\n","\n","class DQN_agent(object):\n","    '''\n","    DQN agent\n","    '''\n","    def __init__(self,\n","                state_no,\n","                act_no,\n","                extra_action_version=0,\n","                replay_memory_capacity=20000,\n","                replay_memory_min_sample=1000,\n","                batch_size=32,\n","                training_period=1,\n","                discount_factor=0.99,\n","                hidden_layers=[64, 32],\n","                learning_rate=0.0001,\n","                epsilon_decay_steps=20000,\n","                update_target_dqn_period=1000, \n","                device=None):\n","\n","        '''\n","        Initialize the DQN agent\n","\n","        :param int state_no: Number of states\n","        :param int act_no: Number of actions\n","        :param int extra_action_version: Mode of choosing action during evaluation phase. Action with maximum value: 0, Raise action instead of Call if possible: 1, Raise action instead of Check if possible: 2, Raise action instead of Fold if possible: 3\n","        :param int replay_memory_capacity: Replay memory size\n","        :param int replay_memory_min_sample: Minimum number of samples in the replay memory during sampling\n","        :param int batch_size: Size of batches to sample from the replay memory\n","        :param int training_period: Train the network in every N steps\n","        :param float discount_factor: Discount factor (gamma) during training the agent\n","        :param list[int] hidden_layers: Dimensions of the hidden layers in the DQN network\n","        :param float learning_rate: The learning rate in the DQN network\n","        :param int epsilon_decay_steps: Number of steps to decay epsilon\n","        :param int update_target_dqn_period: Update target network in every N steps\n","        :param torch.device device: Usage CPU or GPU\n","        '''\n","        \n","        self.replay_memory_min_sample = replay_memory_min_sample\n","        self.update_target_dqn_period = update_target_dqn_period\n","        self.discount_factor = discount_factor\n","        self.epsilon_decay_steps = epsilon_decay_steps\n","        self.batch_size = batch_size\n","        self.act_no = act_no\n","        self.training_period = training_period\n","        self.extra_action_version = extra_action_version\n","\n","        # Torch device on which a torch.Tensor will be allocated\n","        if device is None:\n","            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        else:\n","            self.device = device\n","\n","        # Create the replay memory\n","        self.memory = ReplayMemory(replay_memory_capacity, batch_size)\n","\n","        # Initialize current timestep and current training timestep\n","        self.current_timestep, self.current_training_timestep = 0, 0\n","\n","        # Create array for the epsilon values during the epsilon decay \n","        self.epsilons = np.linspace(1.0, 0.1, epsilon_decay_steps)\n","\n","        # Create the policy and the target network\n","        self.policy_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n","        self.target_dqn = DQN_network(act_no=act_no, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n","\n","        # Set use_raw value for the RLCard environment\n","        self.use_raw = False\n","\n","    def store_and_train(self, transition):\n","        ''' \n","        Save transition into memory and train the agent based on the training period.\n","\n","        :param tuple transition: The transition tuple 'state', 'action', 'reward', 'next_state', 'done'\n","        \n","        '''\n","        (state, action, reward, next_state, done) = tuple(transition)\n","\n","        # Store transition in replay memory\n","        self.memory.push(state['obs'], action, reward, next_state['obs'], done)\n","        # Increment the number of timesteps\n","        self.current_timestep += 1\n","        # Train the agent if the replay memory has data already and agent reached the next training period\n","        time_between = self.current_timestep - self.replay_memory_min_sample\n","        if time_between>=0 and time_between%self.training_period == 0:\n","            self.train()\n","\n","    def discard_invalid_actions(self, action_probs, valid_actions):\n","        ''' \n","        Remove invalid actions and normalize the probabilities.\n","\n","        :param numpy.array[float] action_probs: Probabilities of all action\n","        :param list[int] valid_actions: Valid actions in the current state\n","        :return numpy.array[float] norm_valid_action_probs: Probabilities of valid actions\n","        '''\n","        # Initialize new array\n","        norm_valid_action_probs = np.zeros(action_probs.shape[0])\n","        # Add probability values of valid actions to the array\n","        norm_valid_action_probs[valid_actions] = action_probs[valid_actions]\n","        # Normalize probabilities\n","        norm_valid_action_probs[valid_actions] = 1 / len(valid_actions)\n","        return norm_valid_action_probs\n","\n","    def predict(self, state):\n","        ''' \n","        Predict the action probabilities.\n","\n","        :param numpy.array[float] state: Current state\n","        :return numpy.array[float] q_values: Array of Q values  \n","        '''\n","        epsilon = self.epsilons[min(self.current_timestep, self.epsilon_decay_steps-1)]\n","        actions = np.ones(self.act_no, dtype=float) * epsilon / self.act_no\n","        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state, 0))[0]\n","        best_action = np.argmax(q_values)\n","        actions[best_action] += (1.0 - epsilon)\n","        return actions\n","\n","    def step(self, state):\n","        ''' \n","        Define step function for the RLCard environment.\n","        Get the action for the current state for training purpose.\n","        If neccessary, remove invalid action pobabilities.\n","\n","        :param numpy.array state: The current state\n","        :return int action: The chosen action in the current state\n","        '''\n","        actions = self.predict(state['obs'])\n","        norm_valid_action_probs = self.discard_invalid_actions(actions, state['legal_actions'])\n","        action = np.random.choice(np.arange(len(actions)), p=norm_valid_action_probs)\n","        return action\n","\n","\n","    def eval_step(self, state):\n","        ''' \n","        Define eval_step function for the RLCard environment.\n","        Get the action for the evaluation purpose instead of training purpose.\n","\n","        :param numpy.array state: The current state\n","        :return int action: The chosen action in the current state\n","        '''\n","        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state['obs'], 0))[0]\n","        norm_valid_action_probs = self.discard_invalid_actions(np.exp(q_values), state['legal_actions'])\n","        # Check version of choosing action\n","        if self.extra_action_version == 1:\n","          # If Raise (1) is a valid action and the best action is Call (0)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==0:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        elif self.extra_action_version == 2:\n","          # If Raise (1) is a valid action and the best action is Check (3)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==3:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        elif self.extra_action_version == 3:\n","          # If Raise (1) is a valid action and the best action is Fold (2)\n","          if 1 in state['legal_actions'] and np.argmax(norm_valid_action_probs)==2:\n","            best_action = 1\n","          else:\n","            best_action = np.argmax(norm_valid_action_probs)\n","        else:\n","          best_action = np.argmax(norm_valid_action_probs)\n","        return best_action, norm_valid_action_probs\n","\n","    \n","    def train(self):\n","        ''' \n","        Train the agent.\n","\n","        return float loss: The loss of the current batch\n","        '''\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.batch_size)\n","\n","        # Get best next action using the policy network\n","        q_values_next = self.policy_dqn.get_qvalue(next_state_batch)\n","        best_actions = np.argmax(q_values_next, axis=1)\n","\n","        # Calculate Q values from the target policy\n","        q_values_next_target = self.target_dqn.get_qvalue(next_state_batch)\n","        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * self.discount_factor * q_values_next_target[np.arange(self.batch_size), best_actions]\n","\n","        # Update policy network\n","        state_batch = np.array(state_batch)\n","        loss = self.policy_dqn.update(state_batch, action_batch, target_batch)\n","\n","        # Update target network based on the target update period\n","        if self.current_training_timestep % self.update_target_dqn_period == 0:\n","            self.target_dqn = deepcopy(self.policy_dqn)\n","\n","        self.current_training_timestep += 1\n","\n","\n","    def get_state_dict(self):\n","        ''' \n","        Get the state dictionaries.\n","\n","        :return dict model_dict: Dictionaries containing the whole state of the policy and target modules\n","        '''\n","        model_dict = {'policy_network': self.policy_dqn.DQN_network.state_dict(), 'target_network': self.target_dqn.DQN_network.state_dict()}\n","        return model_dict\n","\n","    def load_networks(self, checkpoint):\n","        ''' \n","        Load network models.\n","\n","        :param dict checkpoint: Checkpoint of the policy and target networks\n","        '''\n","        self.policy_dqn.DQN_network.load_state_dict(checkpoint['policy_network'])\n","        self.target_dqn.DQN_network.load_state_dict(checkpoint['target_network'])\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZK0dotrUla77","executionInfo":{"status":"ok","timestamp":1605611978293,"user_tz":-60,"elapsed":12446,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"d9d960a8-41db-4174-f56c-c710a8a8034b","colab":{"base_uri":"https://localhost:8080/"}},"source":["import rlcard\n","from rlcard import models\n","from rlcard.agents import RandomAgent\n","from rlcard.utils import seeding, tournament\n","from rlcard.utils import Logger\n","import torch\n","import os\n","\n","# Create environments\n","env = rlcard.make('leduc-holdem', config={'seed': 0})\n","eval_env = rlcard.make('leduc-holdem', config={'seed': 0})\n","\n","# Set a global seed\n","seeding.create_seed(0)\n","\n","# Play agressive game based on the version of choosing actual action\n","# Action with maximum value: 0\n","# Raise action instead of Call if possible: 1\n","# Raise action instead of Check if possible: 2\n","# Raise action instead of Fold if possible: 3\n","extra_action_version=1\n","\n","# Opponent agent\n","# Random agent: 0\n","# Pretrained agent with nfsp: 1\n","opponent_agent_version_train=1\n","opponent_agent_version_eval=0\n","\n","# The paths for saving the logs and learning curves\n","log_dir = './experiments/leduc_holdem_dqn_result/'\n","\n","# Create DQN agent\n","agent = DQN_agent(state_no=env.state_shape,\n","                  act_no=env.action_num, \n","                  replay_memory_min_sample=1000,\n","                  training_period=10,\n","                  hidden_layers=[128, 128],\n","                  device=torch.device('cpu'),\n","                  extra_action_version=extra_action_version)\n","\n","# Create opponent agent for training\n","if opponent_agent_version_train == 1:\n","  # Create a pre-trained NFSP agent\n","  opponent_agent_train = models.load('leduc-holdem-nfsp').agents[0]\n","else:\n","  # Create a random agent\n","  opponent_agent_train = RandomAgent(action_num=eval_env.action_num)\n","\n","# Create opponent agent for evaluation\n","if opponent_agent_version_eval == 1:\n","  # Create a pre-trained NFSP agent\n","  opponent_agent_eval = models.load('leduc-holdem-nfsp').agents[0]\n","else:\n","  # Create a random agent\n","  opponent_agent_eval = RandomAgent(action_num=eval_env.action_num)\n","\n","# Add the agent to the environments\n","env.set_agents([agent, opponent_agent_train])\n","eval_env.set_agents([agent, opponent_agent_eval])\n","\n","# Initialize logger\n","logger = Logger(log_dir)\n","\n","# Number of episodes, number of games during evaluation and evaluation in every N steps\n","episode_no, evaluate_games, evaluate_period = 1000, 100, 10\n","\n","for episode in range(episode_no):\n","    # Generate data from the environment\n","    trajectories, _ = env.run(is_training=True)\n","\n","    # Feed transitions into agent memory, and train the agent\n","    for ts in trajectories[0]:\n","        agent.store_and_train(ts)\n","\n","    # Evaluate the performance\n","    if episode % evaluate_period == 0:\n","        logger.log_performance(env.timestep, tournament(eval_env, evaluate_games)[0])\n","\n","# Close files in the logger\n","logger.close_files()\n","\n","# Save model\n","save_dir = 'models/dqn'\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)\n","state_dict = agent.get_state_dict()\n","torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained/leduc_holdem_nfsp/model\n","\n","----------------------------------------\n","  timestep     |  1\n","  reward       |  1.21\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  42\n","  reward       |  1.205\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  85\n","  reward       |  1.04\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  112\n","  reward       |  1.29\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  143\n","  reward       |  0.94\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  181\n","  reward       |  1.45\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  220\n","  reward       |  1.1\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  255\n","  reward       |  1.375\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  288\n","  reward       |  0.785\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  318\n","  reward       |  1.09\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  349\n","  reward       |  1.465\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  381\n","  reward       |  1.02\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  408\n","  reward       |  0.85\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  444\n","  reward       |  1.4\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  462\n","  reward       |  1.1\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  500\n","  reward       |  1.045\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  531\n","  reward       |  1.29\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  564\n","  reward       |  1.385\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  600\n","  reward       |  1.13\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  641\n","  reward       |  1.04\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  667\n","  reward       |  0.825\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  697\n","  reward       |  1.16\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  729\n","  reward       |  1.65\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  752\n","  reward       |  0.705\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  783\n","  reward       |  1.54\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  810\n","  reward       |  0.95\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  844\n","  reward       |  0.47\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  872\n","  reward       |  0.96\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  915\n","  reward       |  0.535\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  952\n","  reward       |  0.97\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  978\n","  reward       |  1.135\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1014\n","  reward       |  1.515\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1052\n","  reward       |  1.03\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1096\n","  reward       |  0.73\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1121\n","  reward       |  1.005\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1151\n","  reward       |  1.34\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1191\n","  reward       |  1.265\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1221\n","  reward       |  1.27\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1252\n","  reward       |  0.54\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1287\n","  reward       |  1.15\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1319\n","  reward       |  1.295\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1356\n","  reward       |  0.77\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1385\n","  reward       |  1.25\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1413\n","  reward       |  1.66\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1447\n","  reward       |  0.995\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1482\n","  reward       |  1.2\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1521\n","  reward       |  1.06\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1559\n","  reward       |  1.21\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1588\n","  reward       |  1.57\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1626\n","  reward       |  1.02\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1658\n","  reward       |  1.315\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1694\n","  reward       |  1.09\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1734\n","  reward       |  1.175\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1769\n","  reward       |  1.33\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1801\n","  reward       |  0.565\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1843\n","  reward       |  0.93\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1881\n","  reward       |  0.645\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1920\n","  reward       |  0.98\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1964\n","  reward       |  1.37\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  1995\n","  reward       |  1.265\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2030\n","  reward       |  1.84\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2064\n","  reward       |  1.505\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2094\n","  reward       |  1.365\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2126\n","  reward       |  1.46\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2163\n","  reward       |  1.29\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2205\n","  reward       |  0.9\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2239\n","  reward       |  1.005\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2271\n","  reward       |  1.185\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2303\n","  reward       |  0.55\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2341\n","  reward       |  1.255\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2377\n","  reward       |  1.045\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2403\n","  reward       |  0.705\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2435\n","  reward       |  1.46\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2473\n","  reward       |  1.47\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2510\n","  reward       |  1.08\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2551\n","  reward       |  1.41\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2583\n","  reward       |  1.22\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2621\n","  reward       |  1.065\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2663\n","  reward       |  1.53\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2695\n","  reward       |  0.79\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2744\n","  reward       |  1.3\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2770\n","  reward       |  1.115\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2800\n","  reward       |  1.31\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2835\n","  reward       |  1.335\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2865\n","  reward       |  1.2\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2904\n","  reward       |  0.87\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2939\n","  reward       |  1.37\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2966\n","  reward       |  0.695\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  2993\n","  reward       |  1.155\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3031\n","  reward       |  1.435\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3059\n","  reward       |  0.975\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3095\n","  reward       |  1.02\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3132\n","  reward       |  1.16\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3174\n","  reward       |  1.37\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3211\n","  reward       |  1.16\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3243\n","  reward       |  1.155\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3281\n","  reward       |  0.605\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3326\n","  reward       |  1.685\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3358\n","  reward       |  1.515\n","----------------------------------------\n","\n","----------------------------------------\n","  timestep     |  3383\n","  reward       |  1.55\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UcTuXaqCnq5B","executionInfo":{"status":"ok","timestamp":1605612169809,"user_tz":-60,"elapsed":713,"user":{"displayName":"Mónika Farsang","photoUrl":"","userId":"03651393462520036310"}},"outputId":"cf4e9195-d970-4e87-ad8d-c980db3323c3","colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["import os\n","import csv\n","from scipy.ndimage.filters import gaussian_filter1d\n","\n","csv_path = os.path.join(log_dir, 'performance.csv')\n","save_path = log_dir\n","\n","\n","def plot(algorithm):\n","    ''' \n","    Read data from csv file and plot the results\n","    '''\n","    import matplotlib.pyplot as plt\n","    with open(csv_path) as csvfile:\n","        print(csv_path)\n","        reader = csv.DictReader(csvfile)\n","        xs = []\n","        ys = []\n","        for row in reader:\n","            xs.append(int(row['timestep']))\n","            ys.append(float(row['reward']))\n","        fig, ax = plt.subplots()\n","        \n","        # Calculate the trendline\n","        z = np.polyfit(xs, ys, 10)\n","        p = np.poly1d(z)\n","        ax.plot(xs, p(xs))\n","\n","        #ax.plot(xs[:-(N-1)], moving_aves, label=algorithm)\n","        ax.set(xlabel='timestep', ylabel='reward', title=algorithm)\n","        ax.legend()\n","        ax.grid()\n","\n","title = 'Leduc Holdem DQN action version: ' + str(extra_action_version) + ', agent training: ' + str(opponent_agent_version_train) + ', agent eval: ' + str(opponent_agent_version_eval)\n","# Plot the learning curve\n","plot(title)\n"],"execution_count":29,"outputs":[{"output_type":"stream","text":["No handles with labels found to put in legend.\n"],"name":"stderr"},{"output_type":"stream","text":["./experiments/leduc_holdem_dqn_result/performance.csv\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"7J1E4-rU8Waz"},"source":[""],"execution_count":null,"outputs":[]}]}